\section{Pr\'elimitaires math\'ematiques : optimisation Lagrangienne}

\subsection{Optimisation et dualit\'e}

Un outil commode, d\'ecrit dans \cite{Lj}, pour la prise de d\'ecision est la s\'eparation lin\'eaire :

\par
Un \'echantillon de donn\'ees $(x_i)$, labell\'ees par une famille $y_i$ dans $\{-1;1\}$ %
\^etre suppos\'ement s\'epar\'e par un hyperplan d'\'equation $\langle w|x\rangle+b=0$, %
c'est-\g{a}-dire que le signe du label $y_i$ \g{a} pr\'edire est le m\^eme que celui de $\langle w|x_i\rangle+b=0$ pour tout $i$.

\par
On recherche, parmi les hyperplans param\'etr\'es par $(w,b)$, ceux ou celui qui maximise la distance de l'hyperplan \g{a} l'\'echantillon $(x_i)$. %
Ceci revient \g{a} maximixer $\frac{1}{\|w\|}$, ainsi le probl\g{e}me d'optimisation de la fonction de d\'ecision revient \g{a} la minimisation de :

\[\frac{\|w\|^2}{2}\text{ sous les contraintes }y_i(\langle w|x_i\rangle -1)\geq 0\]

Ce probl\g{e}me, pour lequel on d\'efinit le lagrangien %
$L:(w,b,\alpha)\mapsto\frac{1}{2}\|w\|^2-\sum\limits_i \alpha_i(y_i\langle x|x_i\rangle-1)$ %
est \'equivalent au probl\g{e}me dual : trouver
\[\alpha^{\ast}:=\arg\min\limits_{\alpha}\sum\limits_i\alpha_i-\frac{1}{2}\sum\limits_{i,j}\alpha_i\alpha_jy_iy_j\langle x_i|x_j\rangle\]
sous $\alpha_i\geq 0$ et $\sum\limits_i y_i\alpha_i=0$.

\par
La fonction de d\'ecision obtenue ainsi est $x\mapsto\sum_i\alpha_iy_i\langle x_i|x\rangle+b^{\ast}$, %
$b^{\ast}$ est calcul\'e \g{a} l'aide du lagrangien, une fois $\alpha^{\ast}$ d\'etermin\'e.

\par
Si l'on relaxe, par ailleurs, l'hypoth\g{e}se de s\'eparabilit\'e lin\'eaire de tout l'\'echantillon en autorisant des erreurs :
\[y_i\langle w|x_i\rangle -1\geq\xi_i,\xi_i<0\text{ pour certains }i\]
on se ram\g{e}ne au probl\g{e}me de m\'inimisation de :
\[\frac{\|w\|^2}{2}+C\sum\limits_i(y_i(\langle w|x_i\rangle +b)-1)_+\]
pour un certaine p\'enalit\'e $C$ sup\'erieur \g{a} $0$.

\par
Le probl\g{e}me admet encore une formulation duale : chercher $\alpha^{\ast}$ qui minimise
\[\sum\limits_i\alpha_i-\frac{1}{2}\sum\limits_{i,j}\alpha_i\alpha_jy_iy_j\langle x_i|x_j\rangle%
\text{ sous }\sum_i\alpha_iy_i=0\text{ et }\alpha_i\in[0,C]\]

\par
La fonction de d\'ecision a la m\^eme forme que pr\'ec\'edemment.

\subsection{Noyaux d\'efinis positifs}

La question de la d\'etermination d'une fonction de d\'ecision se ram\g{e}ne donc \g{a} un probl\g{e}me d'optimisation convexe, %
qui met en jeu la forme quadratique $\alpha \mapsto\sum\limits_{i,j}\alpha_i\alpha_j y_iy_j\langle x_i|x_j\rangle$, %
elle-m\^eme d\'termin\'ee par la famille $(\langle x_i|x_j\rangle)$ des produits scalaires entre l'\'echanltillon $(x_i)$.

\par
On note que les donn\'ees de cet \'echantillon sont toujours des vecteurs, le produit scalaire, qui quantifie leur ressemblance, %
est appel\'e un noyau lin\'eaire pour l'ensemble des donn\'ees. C'est le premier exemple de \emph{noyau d\'efini positif} en SVM.

\par
En pratique, on tente d'apppliquer la m\'ethode pr\'ec\'edente, qui coonvient pour la s\'eparation lin\'eaire, %
\g{a} des jeux de donn\'ees situ\'es dans des ensembles qui ne sont munis a priori d'aucune structure Hilbertienne ou vectorielle.

\par
Il est toutefois possible, pour un jeu de donn\'ees $(x_i)$ contenu dans un espace $E$, d'associer \g{a} chaque donnn\'ee $x_i$, %
donc potentiellement \g{a} chaque \'el\'ement $x$ de $E$ \g{a} mod\'eliser, un \emph{vecteur de repr\'esentation} %
ou \og{} feature vector\fg{} $\Phi (x)$, situ\'e dans un certain espace vectoriel $V$.

\par
On munit 

\par




